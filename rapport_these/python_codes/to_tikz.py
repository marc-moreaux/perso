# train_loss, train_acc, test_loss, test_acc
import matplotlib.pyplot as plt

tab = {0.001*2:{},0.001*1:{},0.001/2:{},0.001/4:{},0.001/8:{}}
for k in tab:
	tab[k] = {8:{},16:{},32:{},64:{}}



lr=0.002000; batch_size=8
tab[lr][batch_size] = [
	[7.1298, 0.5205, 7.6038, 0.5037],
	[7.7104, 0.5030, 6.4874, 0.5448],
	[7.4387, 0.5125, 3.6543, 0.5325],
	[7.0277, 0.5285, 6.5024, 0.5684],
	[4.2035, 0.5490, 2.3728, 0.5450],
	[1.1518, 0.5080, 0.6929, 0.5075],
	[0.6948, 0.4935, 0.6932, 0.4950],
	[0.6953, 0.4675, 0.6931, 0.4963],
	[0.6933, 0.4800, 0.6932, 0.4888],
	[0.6973, 0.4600, 0.6903, 0.5188]
]

lr=0.001000; batch_size=8
tab[lr][batch_size] = [
	[5.5473, 0.5300, 0.7278, 0.4963],
	[1.2739, 0.5290, 0.6919, 0.5112],
	[0.6967, 0.4995, 0.6939, 0.4913],
	[0.6935, 0.5000, 0.7002, 0.5050],
	[0.6937, 0.4990, 0.6931, 0.5012],
	[0.6939, 0.4940, 0.6932, 0.4925],
	[0.6936, 0.4900, 0.6932, 0.5062],
	[0.6932, 0.4940, 0.6932, 0.4875],
	[0.6932, 0.4910, 0.6931, 0.5075],
	[0.6932, 0.4910, 0.6932, 0.4950]
]

lr=0.000500; batch_size=8
tab[lr][batch_size] = [
	[0.8130, 0.5185, 0.6953, 0.5212],
	[0.6785, 0.5490, 0.7170, 0.4938],
	[0.6701, 0.5485, 0.6596, 0.6262],
	[0.6618, 0.5505, 0.6504, 0.5887],
	[0.6448, 0.5780, 0.6062, 0.6542],
	[0.6428, 0.5845, 0.6573, 0.6325],
	[0.6463, 0.5905, 0.6208, 0.7002],
	[0.6361, 0.5895, 0.5959, 0.6412],
	[0.6397, 0.6140, 0.6412, 0.5250],
	[0.6224, 0.6185, 0.5795, 0.6791]
]

lr=0.000250; batch_size=8
tab[lr][batch_size] = [
	[0.8492, 0.5485, 0.6932, 0.5000],
	[0.6735, 0.5745, 0.6797, 0.6375],
	[0.6686, 0.6055, 0.6538, 0.6468],
	[0.6699, 0.6235, 0.6206, 0.6250],
	[0.6598, 0.6110, 0.6404, 0.6555],
	[0.6616, 0.6190, 0.5757, 0.6900],
	[0.6491, 0.6455, 0.6066, 0.6700],
	[0.6392, 0.6355, 0.5801, 0.6965],
	[0.6215, 0.6555, 0.5514, 0.6937],
	[0.6197, 0.6610, 0.5807, 0.6517]
]

lr=0.000125; batch_size=8
tab[lr][batch_size] = [
	[0.7972, 0.5475, 0.7034, 0.5038],
	[0.6741, 0.6085, 0.6862, 0.5487],
	[0.6405, 0.6250, 0.5777, 0.6729],
	[0.6363, 0.6480, 0.5877, 0.6825],
	[0.6240, 0.6630, 0.5482, 0.6663],
	[0.6017, 0.6680, 0.5297, 0.7090],
	[0.5768, 0.6960, 0.6260, 0.6887],
	[0.5600, 0.7010, 0.5797, 0.6816],
	[0.5469, 0.7140, 0.4932, 0.7538],
	[0.5384, 0.7325, 0.6028, 0.6925]
]



lr=0.002000; batch_size=16
tab[lr][batch_size] = [
	[4.5990, 0.5255, 0.7001, 0.4975],
	[0.7239, 0.5115, 0.6935, 0.4963],
	[0.6967, 0.4980, 0.6931, 0.5038],
	[0.6934, 0.5000, 0.6933, 0.4938],
	[0.6933, 0.4930, 0.6933, 0.4863],
	[0.6933, 0.4910, 0.6931, 0.5138],
	[0.6992, 0.5025, 0.6987, 0.5050],
	[0.6952, 0.4945, 0.6931, 0.5012],
	[0.6933, 0.4920, 0.6932, 0.4888],
	[0.6933, 0.4940, 0.6931, 0.5088]
]

lr=0.001000; batch_size=16
tab[lr][batch_size] = [
	[3.4125, 0.5100, 0.6938, 0.5000],
	[0.6971, 0.4940, 0.6929, 0.5150],
	[0.6941, 0.4905, 0.6932, 0.4913],
	[0.7060, 0.4885, 0.6923, 0.4888],
	[0.6935, 0.5010, 0.6931, 0.5112],
	[0.6932, 0.4940, 0.6932, 0.4975],
	[0.6932, 0.4700, 0.6931, 0.5025],
	[0.6932, 0.4770, 0.6932, 0.4988],
	[0.6932, 0.4730, 0.6932, 0.4851],
	[0.6932, 0.4790, 0.6931, 0.5125]
]

lr=0.000500; batch_size=16
tab[lr][batch_size] = [
	[1.3066, 0.5445, 0.6943, 0.5038],
	[0.6754, 0.5855, 0.6340, 0.6194],
	[0.6756, 0.5840, 0.6438, 0.5750],
	[0.6694, 0.5865, 0.5918, 0.6162],
	[0.6500, 0.6195, 0.6659, 0.6542],
	[0.6630, 0.6155, 0.5810, 0.7050],
	[0.6319, 0.6245, 0.6332, 0.6493],
	[0.6348, 0.6455, 0.6218, 0.6188],
	[0.6334, 0.6380, 0.5665, 0.6987],
	[0.6194, 0.6575, 0.5772, 0.6480]
]

lr=0.000250; batch_size=16
tab[lr][batch_size] = [
	[0.9378, 0.5140, 0.6985, 0.5062],
	[0.6929, 0.5475, 0.6924, 0.5124],
	[0.6825, 0.5635, 0.6707, 0.6262],
	[0.6830, 0.5555, 0.6652, 0.6225],
	[0.6736, 0.5795, 0.6657, 0.6766],
	[0.6815, 0.5615, 0.6761, 0.6312],
	[0.6716, 0.5750, 0.6601, 0.6550],
	[0.6645, 0.5930, 0.6675, 0.6592],
	[0.6495, 0.6100, 0.6483, 0.6462],
	[0.6562, 0.6105, 0.6341, 0.7090]
]

lr=0.000125; batch_size=16
tab[lr][batch_size] = [
	[0.7645, 0.5390, 0.7130, 0.4988],
	[0.6669, 0.5945, 0.7882, 0.5025],
	[0.6633, 0.6230, 0.7162, 0.6107],
	[0.6287, 0.6395, 0.6523, 0.6475],
	[0.6068, 0.6710, 0.5786, 0.6600],
	[0.6062, 0.6790, 0.5417, 0.7164],
	[0.5949, 0.6915, 0.6212, 0.6600],
	[0.5725, 0.7100, 0.5648, 0.6953],
	[0.5476, 0.7200, 0.5810, 0.7037],
	[0.5388, 0.7295, 0.5375, 0.7150]
]



lr=0.002000; batch_size=32
tab[lr][batch_size] = [
	[7.6455, 0.5025, 8.0335, 0.5012],
	[7.3565, 0.5000, 5.7929, 0.5675],
	[4.6892, 0.5160, 1.0569, 0.4850],
	[0.6988, 0.4990, 0.6931, 0.5025],
	[0.6933, 0.4920, 0.6932, 0.5000],
	[0.6933, 0.4900, 0.6931, 0.5012],
	[0.6933, 0.4900, 0.6932, 0.4975],
	[0.6933, 0.4870, 0.6932, 0.4950],
	[0.6933, 0.4870, 0.6931, 0.5124],
	[0.6933, 0.4870, 0.6932, 0.4988]
]

lr=0.001000; batch_size=32
tab[lr][batch_size] = [
	[3.5860, 0.4870, 0.6918, 0.4900],
	[0.6953, 0.4925, 0.6932, 0.4975],
	[0.6932, 0.5000, 0.6931, 0.5062],
	[0.6932, 0.4840, 0.6931, 0.5075],
	[0.6932, 0.4630, 0.6931, 0.5138],
	[0.6932, 0.4680, 0.6932, 0.4950],
	[0.6932, 0.4780, 0.6931, 0.5012],
	[0.6932, 0.4760, 0.6931, 0.5000],
	[0.6932, 0.4820, 0.6931, 0.5025],
	[0.6932, 0.4890, 0.6932, 0.4988]
]

lr=0.000500; batch_size=32
tab[lr][batch_size] = [
	[1.2637, 0.5445, 0.6954, 0.4950],
	[0.6815, 0.5805, 0.6825, 0.5435],
	[0.6521, 0.6070, 0.6640, 0.5363],
	[0.6356, 0.6215, 0.6061, 0.6517],
	[0.6284, 0.6420, 0.5970, 0.6700],
	[0.6353, 0.6290, 0.5884, 0.6950],
	[0.6150, 0.6640, 0.5900, 0.6779],
	[0.6091, 0.6710, 0.6028, 0.6562],
	[0.5927, 0.6955, 0.5612, 0.7013],
	[0.5971, 0.6790, 0.5727, 0.6841]
]

lr=0.000250; batch_size=32
tab[lr][batch_size] = [
	[0.8337, 0.5355, 0.7002, 0.4900],
	[0.6711, 0.5785, 0.7445, 0.5037],
	[0.6612, 0.5985, 0.6563, 0.5463],
	[0.6503, 0.6195, 0.5883, 0.6400],
	[0.6586, 0.6335, 0.5605, 0.6754],
	[0.6525, 0.6310, 0.5290, 0.7113],
	[0.6251, 0.6395, 0.5418, 0.6950],
	[0.6365, 0.6445, 0.5381, 0.7139],
	[0.6139, 0.6540, 0.5832, 0.6138],
	[0.5959, 0.6635, 0.5305, 0.6853]
]

lr=0.000125; batch_size=32
tab[lr][batch_size] = [
	[0.9116, 0.5450, 0.7070, 0.4950],
	[0.6814, 0.5965, 0.8110, 0.5025],
	[0.6555, 0.6170, 0.7092, 0.6169],
	[0.6471, 0.6320, 0.6372, 0.6362],
	[0.6181, 0.6420, 0.5723, 0.7052],
	[0.5949, 0.6775, 0.5388, 0.7238],
	[0.5910, 0.6825, 0.5619, 0.7212],
	[0.5827, 0.6975, 0.5419, 0.7475],
	[0.5570, 0.7130, 0.5239, 0.7562],
	[0.5583, 0.7170, 0.5437, 0.7325]
]



lr=0.002000; batch_size=64
tab[lr][batch_size] = [
	[3.3713, 0.5275, 0.6929, 0.5025],
	[0.6940, 0.5140, 0.6845, 0.5212],
	[0.6751, 0.5670, 0.6218, 0.6269],
	[0.6642, 0.5845, 1.0489, 0.5625],
	[0.6613, 0.5870, 0.6784, 0.5437],
	[0.6631, 0.6080, 0.6739, 0.5510],
	[0.6484, 0.6130, 0.6129, 0.6250],
	[0.6522, 0.5960, 0.6563, 0.5850],
	[0.6349, 0.6135, 0.6731, 0.6269],
	[0.6305, 0.6255, 0.5800, 0.6562]
]

lr=0.001000; batch_size=64
tab[lr][batch_size] = [
	[7.8181, 0.5035, 7.9910, 0.4988],
	[5.8825, 0.5100, 0.7059, 0.5375],
	[0.7510, 0.5085, 0.6932, 0.4850],
	[0.6960, 0.5095, 0.6632, 0.6007],
	[0.6911, 0.5290, 0.6728, 0.5625],
	[0.6844, 0.5435, 0.6472, 0.6144],
	[0.6825, 0.5695, 0.6511, 0.5938],
	[0.6782, 0.5620, 0.6704, 0.5825],
	[0.6817, 0.5600, 0.6390, 0.6269],
	[0.6681, 0.5690, 0.6360, 0.6350]
]

lr=0.000500; batch_size=64
tab[lr][batch_size] = [
	[1.2328, 0.5485, 0.7091, 0.5050],
	[0.6866, 0.5850, 0.7774, 0.5050],
	[0.6606, 0.5910, 0.6301, 0.6238],
	[0.6513, 0.5985, 0.5970, 0.6306],
	[0.6270, 0.6165, 0.5782, 0.6637],
	[0.6328, 0.6345, 0.6027, 0.6900],
	[0.6118, 0.6455, 0.5843, 0.6741],
	[0.6192, 0.6385, 0.5689, 0.6987],
	[0.6018, 0.6650, 0.5402, 0.7325],
	[0.6060, 0.6420, 0.5443, 0.6965]
]

lr=0.000250; batch_size=64
tab[lr][batch_size] = [
	[0.9261, 0.5545, 0.6993, 0.5138],
	[0.6709, 0.5955, 0.6938, 0.5124],
	[0.6528, 0.6110, 0.6177, 0.6338],
	[0.6343, 0.6310, 0.5565, 0.6837],
	[0.6187, 0.6220, 0.5207, 0.7276],
	[0.6088, 0.6430, 0.5524, 0.7400],
	[0.6118, 0.6490, 0.5351, 0.7090],
	[0.5893, 0.6680, 0.5563, 0.6975],
	[0.5840, 0.6760, 0.5798, 0.7063],
	[0.5620, 0.6900, 0.5470, 0.7376]
]

lr=0.000125; batch_size=64
tab[lr][batch_size] = [
	[0.8002, 0.5600, 0.6935, 0.4988],
	[0.6716, 0.5960, 0.6849, 0.5463],
	[0.6464, 0.6255, 0.6213, 0.6356],
	[0.6396, 0.6365, 0.5941, 0.6925],
	[0.6348, 0.6510, 0.5425, 0.7301],
	[0.6120, 0.6615, 0.6218, 0.6813],
	[0.6132, 0.6600, 0.6109, 0.7125],
	[0.6054, 0.6825, 0.5598, 0.7015],
	[0.5811, 0.6875, 0.5719, 0.7312],
	[0.5798, 0.6945, 0.5508, 0.7312]
]


import numpy as np


def to_tikz_plot(lines, idx=1):
	"""
	lines -- a 2D array with the (x,y) coordinates of many lines
	idx -- 1 for loss
	       2 for accuracy
	"""
	# Add one line on the plot (many (x,y) )
	print "\\addplot plot coordinates {"
	for x,line in enumerate(lines) :
		y = line[idx]
		print "(%d,%0.4f)"%(x+1,y),
	print "};"

def to_tikz_figure(lr=-1, batch_size=64, idx=1):
	print "\\begin{tikzpicture}\n\t\\begin{axis}[xlabel=epoch,ylabel=Loss]"
	
	# Plot all lines in a figure (one line per lr)
	if lr == -1:
		for lr in (0.001*2,0.001*1,0.001/2,0.001/4,0.001/8):
			to_tikz_plot(tab[lr][batch_size], idx=1 )
 	
	if batch_size == -1:
		for lr in (8,16,32,64):
			to_tikz_plot(tab[lr][batch_size], idx=1 )
	print 	"\t\\end{axis}\n\\end{tikzpicture}"

my_losses = lambda lr, b_size : [t[0] for t in tab[lr][b_size]]


fig, axs = plt.subplots(2,2)
batch_size = 8
xs = range(10)
for ax,batch_size in zip([b for a in axs for b in a],
	                     (8,16,32,64)):
	for lr in (.002,.001,.0005,.00025,.000125):
		ax.plot(xs, my_losses(lr, batch_size), label=str(lr))
	
	ax.set_xlabel(str(batch_size))
	ax.legend()
	ax.set_ylim((.4,1.5))



plt.show()

