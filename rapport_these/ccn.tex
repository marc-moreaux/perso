\chapter{Saliency Through Deep Nets} % (fold)
\label{sec:saliency_through_deep_nets}
	
	In this chapter we review the contribution we made on deep learning and contextualize it into our problematic.

	\section{Context of the research}
	\label{sec:context_of_the_research}

		As a reminder, the aim of the thesis is to perform Audio-visual perception for human action recognition and to recognize object affordances from an assistant robot. The research direction we consider to tackle this problem is close-to-real-time semi-supervised learning. To be more specific, we want to teach a classifier to localize human actions, both in time and space, from an incomplete dataset. 	

		\subsection{Video Dataset}
		\label{sub:dataset}
			Going through the existing video datasets, we realized that none of the video datasets had the desired properties of our problematic. The video datasets would be soundless, filmed from high grounds (e.g. video surveillance), filmed with lots of constrains (e.g. same, or very few, people performing actions) and labeling without spatial information. Yet, in September 2016, Youtube released a new database \cite{abu2016youtube} composed by 8 million videos and 4800 classes. All the videos contains a sound band, where filmed from diverse point of views and are very diverse. Regrettably, even for a human (like myself), it's not always obvious to recognize the desired labels of a video, yet, we can expect to find some patterns throughout these classes. One idea could be to shrink this dataset into another one with our desired classes.


		\subsection{Image Dataset}
		\label{sub:image_dataset}
			Before tackling the spatio-temporal analysis of a video stream, we started our researches on spatial analysis of frames (or images). Obviously, these dataset would composed by images displaying objects usually used by humans. As we also want to evaluate the localization abilities of our classifier, we also needed our datasets to contain localization labeling. With respect to this constrains, we selected three datasets : Microsoft coco \cite{lin2014microsoft}, ILSVRC2012 \cite{ILSVRC15} and Pascal VOC 2007 \cite{pascalVoc2007}.



		\subsection{Supervised Learning}
		\label{sub:supervised_learning}
			The first methods we've been looking into was fully-supervised deep-learning, meaning that we would classify our dataset with a neural network and the objective function had as parameters (a) the input image, (b) its class and (c) the localization of this class. Here we found 3 papers :
			\begin{itemize}
				\item \textbf{R-CNN\cite{girshick2014rich} :} In this paper, the authors would perform a selective search on the input image, resulting in 2K images (extracted from the original one). They would then run this images into a convolutional net to obtain a large feature vector and classify these regions with a SVM. Because of the selective search and the propagation of the large amount of images in the network, this implementation wasn't real time, still its mAP reached $66\%$ on VOC2007.
				\item \textbf{Faster R-CNN\cite{ren2015faster} : } In this other paper, Ren et al. would tackle the same problem with a similar approach but they roughly switched the convolution and the selective search. As a result a single image would be processed by a CNN which would produce a feature map. This map would be analyzed by a region proposal network to create region proposals and followed by a classifier to produce the combination of classes and regions. This method was faster (198ms) and its mAP close to $79\%$ on VOC2007.
				\item \textbf{YOLO\cite{redmon2015you} :} Here, Redmon et al. took a different approach on the problem. They would split the original image into 49 equal squares and guess, for each of them a bounding box (which could be bigger than the square), this bounding box probability (often low) and a probability for the class in this square. Given these many values, they would aggregate the results into a prediction. With this simpler architecture, the authors reached $78.6\%$ mAP on VOC2007 and a speed of 25ms.
			\end{itemize}

		\subsection{Semi-Supervised Learning}
		\label{sub:semi_supervised_learning}
			The methodology we prefer to answer our problematic is "deep semi-supervised learning". As we've seen earlier, there is plenty of video datasets that exists, which we can aggregate, but none of them have a spatio-temporal labeling of the classes. What they have instead is a class labeling for the video. What we expect from our work is to localize an unspecified pattern under each and every class which would discriminate a class from another one and which, hopefully, would correspond to a class localization. In their work, Zhou et al. \cite{zhou2015learning} use such technique. Their learning algorithm is a function of two inputs : (a) the image and (b) the class it belongs to. At run time, the classifier is able to predict, close to real-time, a class and its position on the image. Sadly, there wasn't any time or accuracy evaluation on VOC2007 comparison. 
			What we found in the afford mentioned paper, is (a) the model always predict a class, though our use cases won't always need to see an action. (b) The Tensorflow implementation is sub-optimal and won't work real-time. Still, we believe their technique, based on summed of convolved images (namely : Global Average Polling (GAP)), is very efficient and should be further investigated.


	
	\section{Work on saliency}
	\label{sec:work_on_saliency}

		\subsection{Global Average Pooling}
		\label{sub:global_average_pooling}
			In this section, we want to emphasize the benefits of Global Average Pooling (GAP) as formulated and used in Zhou et al.'s work \cite{zhou2015learning}. GAP is a simple operator meaning a two dimensional tensor into a value : the mean of this 2D tensor. It was used the following way : Consider a stack of 2D convolutions over an image resulting in a 3 dimensional tensor. The GAP operator, applied to this last tensor, would result in a one dimensional tensor which would correspond to the means of each activation unit. Lets $f_k(x,y)$ represent an activation of unit $k$ at spatial location $(x,y)$.
			\begin{equation}
				\text{CAM}_k = \sum_{x,y}f_k(x,y)
			\end{equation}

			Because of its formulation, the GAP layer is position agnostic. The x's and y's localizations are lost. Thankfully, we can record the 3D tensor preceding the GAP layer and use it to render what Zhou et al. called the Class Activation Mapping, a mapping where we would see the activity of the class.

		\subsection{Class Activation Mapping}
		\label{sub:class_activation_mapping}
			After the GAP, Zhou et al. placed a single neuron for each class $s_c$ which were feed to the softmax function. 
			\begin{equation}
				s_c = \sum_k w_k^c \cdot \text{CAM}_k
			\end{equation}
			The $w_k^c$ weight correspond to the importance a $f_k$ unit has in recognizing a class. Multiplying one with the other, we can reconstruct a heat-map indicating where was the class active :
			\begin{equation}
				M_c = \sum_k w_k^c \cdot f_k(x,y)
			\end{equation}

		\subsection{Issues}
		\label{sub:issues}
			Much of this work is very satisfying. The classifier is able to detect classes (the learning task) plus is able to provide a saliency map (without location labeling). Yet, we found two issues : 
			\begin{itemize}
				\item the first one is related to the messiness of the prediction when none of the learned classes are present. This may sound silly as the classifier isn't supposed to deal with this type of situation, yet, in our use case, we would like our classifier to be insensible to images with none of the target classes. 
				\item The second issue is more of a conceptual one. By Leveraging the $\text{CAM}_x$ values and aggregating them into a single neuron, the classifier looses some spatial dependencies. 
				For the sake of argument, lets consider the $f_1$ unit was computing the probability of face being present in an image and $f_3$ was computing the probability of a chest being present. These two probabilities are a function of a given amount of xs and ys pixels, therefore, they are 2D localized. Using a weighted neurons after the CAM layer removes the localization relation existing between the chest and the face.
			\end{itemize}
			In the following subsection, we will see how did we addressed these two issues.

		\subsection{Second issue}
		\label{sub:second_issue}		
			To address these two issues we run experiments. We first addressed the conceptual one by comparing a regular conv-FC net (a network composed by a stack of convolutional layers followed by fully connected layers and a softmax) to a conv-CAM net (a network composed by a stack of convolutional layers followed by a CAM layer and a softmax). 
			
			The idea was to confirm that the fully connected layer(s) at the end of the Conv-FC were forcing the network to over-fitting the x and y positions of a class while training on a given dataset. Concretely : consider we have to train a classifier to recognize beds and the beds images on the database only shows beds on the lower left corner of these databases. Considering this training setup, we hypothesize that if, at test time, we show to the classifier images with a bed on the top right corner, the classifier won't be able to classify this new sample as a bed.

			We could prove this property (a) mathematically or (b) In practice.

			\textbf{(a) Mathematically}, the intuition would be that the gradient of the target class would never have high values at the "non-active" regions. Considering a Conv-FC net with a single Fully-Connected layer, we would see that the gradients of the detected softmax class would be multiplied to the output values of the last convolutional layer which would be high if their range of attention contains a bed, and low otherwise. With the training algorithm we use (stochastic gradient descent) these low gradients won't permit a class detection where the training set didn't show the desired pattern (a bed elsewhere than the lower left corner to mention our previous example).

			\textbf{(b) In practice}, we've tried to support our argument with an experiment. The first step was trying whether a Conv-FC net was going to over-fit to a given position on a constrained dataset. To do this, we simulated a training MNIST dataset \ref{sub:constrained_position_mnist} with images of size $(100*100)$ and the digits at a position $(20*20)$ (right figure of \fref{fig:CP_MNIST_5050_2020_noisy}). At test time, we used a dataset with different digits positions. "Obviously" those images were only correctly classified when the digits were at $(20*20)$. 

			On the following section, we further detail our experiments.

		\subsection{Experiments}
		\label{sub:experiments}
			In this section, we describe the experiments and their results. 

			\textbf{Test-sets :} These results will be composed by two key values which are the Test-Set accuracy "As Trained" (TSAT)(with a digit at the top-left position) and the Test-Set accuracy "With Random Positions" (TSWRP). On the first case TSAT is constructed like the train-set (same image size, same image position, same noise) but the digits are different. On the second case the TSWRP is constructed likewise but the position of the image is selected at random. 
			When nothing is mentioned, we consider the TSAT to be build with the following parameters : $w^{cp}=100,h^{cp}=100, x^{cp}=10,y^{cp}=10$ and a background equals to ten percent of a noise under a uniform law.

			\textbf{Con-nets :}
			Earlier, we mentioned the Conv-FC and the Conv-CAM, most of the times, the convolutional part of these network is as follow :
			\begin{itemize}
				\item conv1\_1 $[3*3*1*16]$ \footnote{[filter height, filter width, input channels, output channels]}
				\item conv1\_2 $[3*3*16*16]$
				\item max-pool
				\item conv2\_1 $[3*3*16*16]$
				\item conv2\_2 $[3*3*16*16]$
				\item max-pool
				\item conv3\_1 $[3*3*16*16]$
			\end{itemize}
			\textbf{Learning parameters :} When not mentioned, we understand that the learning rate of the network is $0.005$, and the optimizer is the default Tensorflow implementation of "RMSprop".

			\textbf{visualization :} To visualize our results, we use randomly chose a digit and create 64 images with the digit at 64 different positions (in the 100\*100 image)\footnote{(5,5)(5,10)(5,15)(5,20)...(10,5)(10,10)(10,15)...(40,40)}. We will refer to this visualization as the multiPos graph.

			\subsubsection{Conv-FC} 
			\label{ssub:conv_fc}
				The conv-FC network has the convolutional structure aforementioned with a single fully-connected layer on top.
				After 10 epochs, the classifier can recognize the top left digits with 98.88\% accuracy on the TSAT but can't recognize digits randomly placed on an image : the TSWRP accuracy is 11.23\%. On the multiPos graph, we see that the digit "6" is never recognized on the 64 images but when the digit was positioned at (10,10).
				It's straight forward that this model couldn't generalize the position of a digit.

			\subsubsection{Conv-CAM}
			\label{ssub:conv_cam}
				The Conv-CAM net has the convolutional structure aforementioned followed by a CAM layer directly connected the softmax layer. 
				After 10 epochs, the accuracy on TSAT is $97.3\%$ and $97.24\%$ on TSWRP. We also see from the multiPose graph a that the randomly generated digit was recognized at every positions.

			
			

				\begin{figure}[h]
					\centering
					\includegraphics[width=0.45\textwidth]{images/multiPosGraphConvFC}
					\includegraphics[width=0.45\textwidth]{images/multiPosGraphConvCAM}
					\caption{on the left : CP-MNIST generated with : $w^{cp}=50,h^{cp}=50, x^{cp}=20,y^{cp}=20$ and $bg=.1$. and on the right : CP-MNIST generated with : $w^{cp}=100,h^{cp}=100, x^{cp}=20,y^{cp}=20$ and $bg=.1$}
					\label{fig:CP_MNIST_5050_2020_noisy}
				\end{figure}

			




			\subsection{Constrained position MNIST}
			\label{sub:constrained_position_mnist}
				We developed a simulated dataset which we refer by Constrained-position MNIST (CP-MNIST). It's simulated in the sense that it's live generated by a piece of code. The main idea is to generate an image that is going to be of a fixed size (224*224 to fit to the literature) and containing an MNIST digit at a position we set $(x^{cp},y^{cp})$. Because we generate this dataset, we have full information on the class label and on the position label. We can further extend this virtual dataset with many informations such as (a) many MNIST sample on one image (e.g. to render numbers) or (b) a video composed by a series of frame (e.g. to render a letter displacement on a 2 or 3 dimensional space). 
				In order to compute this dataset, a few parameters are available :
				\begin{itemize}
					\item The position of the digit : which can be set arbitrarily or randomly with $(x^{cp},y^{cp})$.
					\item The Background size : which is typically set to $(50*50)$ or $(100*100)$ with $(w^{cp},h^{cp})$.
					\item The background type : which can be either zeros (white background) or noisy (random noise from a normal distribution with a parametrized maximum value : $bg$).
				\end{itemize}
				In \fref{fig:CP_MNIST_5050_2020_noisy}, you can see extracts of this virtual dataset.
				\begin{figure}[h]
					\centering
					\includegraphics[width=0.45\textwidth]{images/CP_MNIST_5050_2020_noisy} 
					.
					\includegraphics[width=0.45\textwidth]{images/CP_MNIST_100100_2020_noisy}
					\caption{on the left : CP-MNIST generated with : $w^{cp}=50,h^{cp}=50, x^{cp}=20,y^{cp}=20$ and $bg=.1$. and on the right : CP-MNIST generated with : $w^{cp}=100,h^{cp}=100, x^{cp}=20,y^{cp}=20$ and $bg=.1$}
					\label{fig:CP_MNIST_5050_2020_noisy}
				\end{figure}

			
			
			\subsubsection{Regularization term}
			\label{ssub:regularization_term}


			\subsubsection{All Convolutional Net}
			\label{ssub:all_convolutional_net}

			
			


	In this paper, we want to emphasize the use CAM layers for their saliency and localization properties.
	The network we use is therefore a variant to the original network zhou2015learning. In the original paper, the authors introduce a combination of Global Average Pooling, followed by a weighted average and a softmax to classify their images. Here, we investigate the use of Global Average Pooling directly followed by a softmax function to classify our data. With this structure, this new network should capture further informations on spatial relations that the original could not grasp.

	\subsection{Methodology} % (fold)
	\label{sub:methodology}

		Deploy a VGG16 with CAM 
		Deploy a VGG16 with CCN + regularizer + multiple CCN sizes
		Compare each of them on pascal VOC

	\subsection{Results:} % (fold)
	\label{sub:results_}
		
		\begin{itemize}
			\item The detector is position agnostic (Vs FCNN) from training to testing.
			\item Can be use with variable image shapes.
			\item May be use for "one shot" learning of given classes.
		\end{itemize}

	A general comment we can make on both the original model and ours, is that both of the models are training-position-agnostic. What we mean here is that both networks can generalize the concept of an object independently of their position in the training set. If we consider a classification task where the network has to classify shoes against many other other classes, and a set of images randomly chosen from the web, it's very likely that all the images display these shoes at the bottom of the image.\\
	The learned distribution (by the network), and a-priori would be fine, if we were to test our networks on the same type of dataset (randomly chosen from the web) but could present weaknesses if we were to test this network on a dataset without this probability distribution (eg: pictures grabed from a mobile platform, like a humanoid robot, or a fying drone).
